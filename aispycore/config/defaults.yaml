# aispycore default configuration

data:
  target_col: "mRS_Label"
  image_col: "path"
  id_col: "id"
  batch_size: 10
  num_parallel_calls: 4
  cache: false

augmentation:
  enabled: false
  transforms: []

model:
  # Pyment backbone: "BinarySFCN" | "RegressionSFCN"
  backbone: "BinarySFCN"
  # Task drives head-swapping logic and evaluator selection.
  # "binary_classification" | "regression"
  task_type: "binary_classification"
  dropout: 0.5
  weight_decay: 1.0e-5
  # Path to pyment pre-trained weights. null = random init.
  backbone_weights: null
  # For RegressionSFCN + regression only. e.g. [0, 6] for mRS.
  prediction_range: null

training:
  epochs: 100
  learning_rate: 1.0e-4

  # Seed is set programmatically from CLI in train.py.
  # null here means "not yet set"; train.py always writes the seed used.
  seed: null

  # "sgd" | "adam"
  optimizer: "sgd"
  loss_fn: "binary_crossentropy"
  metrics: ["accuracy"]
  # "val_balanced_accuracy" | "val_f1" | "val_recall" | "val_specificity"
  custom_val_metrics: ["val_balanced_accuracy", "val_f1"]

  # --- Early stopping ---
  # Master switch — set false to disable EarlyStopping in build_callbacks().
  # GradualUnfreezingTrainer always adds its own per-segment EarlyStopping.
  use_early_stopping: true
  # Keras metric name to monitor ("val_loss", "val_accuracy", etc.).
  # Used by build_callbacks() and KerasTuner.
  early_stopping_metric: val_f1   # or val_f1

  # "min" (lower is better) | "max" (higher is better)
  early_stopping_mode: max
  early_stopping_patience: 10
  early_stopping_min_delta: 0.0

  # --- LR reduction on plateau ---
  reduce_lr_patience: 5
  reduce_lr_factor: 0.5
  min_lr: 1.0e-7

  # --- Stepwise LR scheduler ---
  # When true, LR is divided by 3 at epochs 20, 40, and 60.
  use_stepwise_lr: false

  # --- Epoch-level logging ---
  # When true, EpochLogger writes per-epoch loss, metrics, RAM, VRAM to file.
  log_epoch_metrics: true

  # --- Custom trainers ---
  trainer_type: "standard"
  trainer_kwargs: {}
  # Example for gradual unfreezing:
  #   trainer_type: "gradual_unfreezing"
  #   trainer_kwargs:
  #     blocks:
  #       - [0, 1, 2]      # earliest layers — unfrozen last
  #       - [3, 4, 5, 6]
  #       - [7, 8, 9]      # output-side — unfrozen first
  #     min_delta: 0.001

cross_validation:
  n_folds: 4
  n_repeats: 1
  # If empty, StratificationManager falls back to data.target_col.
  stratify_cols: ['mRS', 'lesion_volume']
  priority_weights: null
  val_size: 0.2
  n_inner_folds: 4
  final_val_split: 0.3
  binning_threshold: 10

tuning:
  enabled: false
  pretuned_hyperparameters:
    - model.dropout: 0.55          # fold 0
      training.learning_rate: 0.0001873
      model.weight_decay: 0.0
    - model.dropout: 0.55          # fold 1
      training.learning_rate: 0.0002828
      model.weight_decay: 0.0002
    - model.dropout: 0.1          # fold 2
      training.learning_rate: 0.00014
      model.weight_decay: 0.00095
    - model.dropout: 0.45          # fold 3
      training.learning_rate: 3.8275703375199456e-05
      model.weight_decay: 0.00074

  backend: "keras_tuner"
  max_trials: 20
  # Keras metric name the tuner optimises — must match a metric produced
  # during model.fit() (e.g. "val_loss", "val_accuracy").
  objective: "val_f1"
  objective_direction: max        # <-- new
  # Epochs per tuning trial (shorter than full training epochs).
  tuning_epochs: 10
  # --- CustomBayesianOptimization scoring ---
  # Window used for mean-objective and improvement-rate calculation.
  last_n_epochs: 3
  # 0.0 = rank by mean objective only. 1.0 = rank by improvement rate only.
  improvement_weight: 0.5
  # Penalise trials where the objective ever increased between epochs.
  exclude_increasing_loss: true

inference:
  embedding_layer_name: "top_pool"  # substring match — null to disable
  batch_size: 10                  # null = use data.batch_size

output:
  save_best_only: true
  monitor_metric: "val_f1"
  monitor_mode: "max"